# -*- coding: utf-8 -*-
"""CustomerChurn_Assignment3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Sti75kJ_NpQeegLmJRsYW9fdeK6ZO12k
"""

#importing all needed libraries
import os
import sklearn
import numpy as np
import pandas as pd
import tensorflow as tf
import seaborn as sns
from keras import Input, Model
from keras.layers import Dense
import matplotlib.pyplot as plt
from keras.optimizers import Adam
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.metrics import accuracy_score
from sklearn.model_selection import StratifiedKFold, GridSearchCV

from google.colab import drive
drive.mount('/content/drive')

"""# Data Collection"""

churn_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/CustomerChurn_dataset.csv')

churn_data.info()

churn_data.head(10)

churn_data['OnlineBackup']

churn_data.describe()

#Total charges had some empty spaces which wasnt reflecting in the info, this code is to replace all those with nan.
churn_data['TotalCharges'].replace(' ', np.nan,inplace=True)
churn_data.info()

#changing the datatype of Totalcharges to be able to use the median in imputer
churn_data['TotalCharges'].astype(float)

"""# **Explorative Data Analysis**"""

#Creating a pie chart to check the likelihood of a customer churning
c_counts=churn_data['Churn'].value_counts()

# Sample data
titles=["Yes","No"]
plt.pie(c_counts, labels=titles, autopct='%1.1f%%', startangle=90,colors= ['#4CAF50', '#8BC34A'])
plt.title('Likelihood of Customer Churn')
plt.legend(loc='upper left', labels=titles)
plt.axis('equal')
plt.show()

"""According to the pie chart, 73.46% of customers are likely not to churn and and 26.54% are likely to churn."""

#using seaborn to analyse the categorical features

fig, ax = plt.subplots(3, 3, figsize = (20, 15))

plt.suptitle('Count plot for features', fontsize = 30, color= 'teal')

diag1 = sns.countplot(x ='Contract', data= churn_data, hue= 'Churn', ax= ax[0, 1], palette= 'autumn')
diag1.set(xlabel = 'Contract')

diag2 = sns.countplot(x ='PaymentMethod', data= churn_data, hue= 'Churn', ax= ax[0, 0], palette= 'spring')
diag2.set(xlabel = 'Payment Method ')

diag3 = sns.countplot(x ='TechSupport', data= churn_data, hue= 'Churn', ax= ax[0, 2], palette= 'Accent')
diag3.set(xlabel = ' Tech Support')

diag4 = sns.countplot(x ='OnlineSecurity', data= churn_data, hue= 'Churn', ax= ax[1, 0], palette= 'copper_r')
diag4.set(xlabel = ' Online Security')

diag5 = sns.countplot(x ='OnlineBackup', data= churn_data, hue= 'Churn', ax= ax[1, 1], palette= 'winter')
diag5.set(xlabel = 'Online Backup')

diag6 = sns.countplot(x ='InternetService', data= churn_data, hue= 'Churn', ax= ax[1, 2], palette= 'summer')
diag6.set(xlabel = ' Internet Service ')

diag7 = sns.countplot(x ='gender', data= churn_data, hue= 'Churn', ax= ax[2, 0], palette= 'binary')
diag7.set(xlabel = 'gender ')

diag8 = sns.countplot(x ='StreamingMovies', data= churn_data, hue= 'Churn', ax= ax[2, 1], palette= 'plasma')
diag8.set(xlabel = 'Streaming Movies')

diag9 = sns.countplot(x ='Partner', data= churn_data, hue= 'Churn', ax= ax[2, 2], palette= 'bone')
diag9.set(xlabel = 'Partner')

plt.tight_layout()
plt.show()

"""According to the plot,
1. Customers that do not stream movies at all are more likely to churn that those that do and those that cant due to inaccessibility to internet service.

2. Customers that have no device protection ,tech support and online backup are also more likely to churn.

3. Also, customers with fiber optic internet service are more likely to churn.

4. Customers with no partners are more likely to churn.
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'churn_data' is your DataFrame
# Set up the matplotlib figure
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))

# Create histograms using Seaborn for 'Churn' categories
sns.histplot(churn_data[churn_data['Churn'] == 'No']['tenure'], bins=15, kde=True, ax=axes[0], color='salmon', label='Churn: No')
axes[0].set_title('Histogram for Tenure')

sns.histplot(churn_data[churn_data['Churn'] == 'Yes']['tenure'], bins=15, kde=True, ax=axes[0], color='teal', label='Churn: Yes')

sns.histplot(churn_data[churn_data['Churn'] == 'No']['MonthlyCharges'], bins=15, kde=True, ax=axes[1], color='salmon', label='Churn: No')
axes[1].set_title('Histogram for Monthly Charges')

sns.histplot(churn_data[churn_data['Churn'] == 'Yes']['MonthlyCharges'], bins=15, kde=True, ax=axes[1], color='teal', label='Churn: Yes')

for ax in axes:
    ax.legend()
plt.show()

"""It can be seen from the histogram that the more years a customer stays with the company, they are less likely to churn.

# **Data Preprocessing**
"""

#we will drop customerID because it would not be necessary in our analysis
churn_data.drop('customerID', axis = 1, inplace = True)

Y = pd.DataFrame()
Y = churn_data['Churn'] # separate the churn column as the Y (Target)
churn_data.drop('Churn', axis = 1, inplace = True) # drop churn
Y

#label encoding our target variable
label_encoder = LabelEncoder()
Y= label_encoder.fit_transform(Y)

Y

#we will impute the using median
imp = SimpleImputer(strategy = 'median')
to_impute = ['TotalCharges']
churn_data[to_impute] = imp.fit_transform(churn_data[to_impute])
churn_data.info()

#encode the object data using the label encoder
encoded_columns=churn_data.copy() #creating a copy of the churn_data to work with
for c in churn_data.select_dtypes("object").columns:
  label_encoder = LabelEncoder()
  encoded_columns[c] = label_encoder.fit_transform(churn_data[c])
encoded_columns.info()

encoded_columns.head()

encoded_columns['Churn'] = Y
print(encoded_columns.info())

#removing overall from the training data
Y = encoded_columns['Churn'] # extract overall
encoded_columns.drop('Churn', axis = 1, inplace = True) # drop overall
encoded_columns.info()

# scaling the data
sc = StandardScaler()  # scaling the data set to be used in the model
training_x = sc.fit_transform(encoded_columns)
training_x = pd.DataFrame(training_x, columns = encoded_columns.columns)
training_x.head(5)



import pickle
with open('scalar_model.pkl', 'wb') as file:
    pickle.dump(sc, file)

training_x

"""# **Selecting Relevant Features**"""

#Training using rf for Feature Importance
model=RandomForestClassifier()
model.fit(training_x,Y)

#Getting the Important Features from our dataset
name_of_feature=training_x.columns #contains the names of features in our dataset
feature_importance=model.feature_importances_ #contains the scores of each feature

#Sorting the Feature Importance
feature_importance_df=pd.DataFrame({'Feature':name_of_feature,'Importance':feature_importance}) #creates a dataframe with the names of our feature and corresponding scores
feature_importance_df=feature_importance_df.sort_values(by='Importance',ascending=False) #sorts our results from highest to lowest
feature_importance_df

#Using the first 9 features
first_nine_features = feature_importance_df['Feature'].values[:9]
first_nine_features
#the model was tested using 10 features intially but the auc score proved better on 9 features

"""# Scaling for App"""

#scaling
sc = StandardScaler()  # scaling the data set to be used in the model
training_s = sc.fit_transform(encoded_columns[first_nine_features])
training_s

import pickle
with open('scaled_model.pkl', 'wb') as file:
    pickle.dump(sc, file)

#updates to our new training_x value
training_x = training_x[first_nine_features]

"""# **Model Building**"""

from sklearn.model_selection import train_test_split
Xtrain,Xtest,Ytrain,Ytest=train_test_split(training_x,Y,test_size=0.2,random_state=42) #splitting our data
x_validate,x_test,y_validate,y_test=train_test_split(Xtest,Ytest,test_size=0.5,random_state=42)

#using functional API
input_layer = Input(shape=(Xtrain.shape[1],))
#hidden layers
first_hidden = Dense(25, activation='relu')(input_layer)
second_hidden = Dense(20, activation='relu')(first_hidden)
third_hidden = Dense(15, activation='relu')(second_hidden)
#output layer
output_layer = Dense(1, activation='sigmoid')(third_hidden)

#model creation
model = Model(inputs=input_layer, outputs=output_layer)

model.compile(loss='binary_crossentropy',
              optimizer=Adam(learning_rate=0.0001),
              metrics=['accuracy'])

train_hist = model.fit(Xtrain,Ytrain,epochs=100,batch_size=32,validation_data=(x_validate, y_validate),verbose=1)

loss,accuracy = model.evaluate(Xtest, Ytest, verbose=0)

#Test Loss
print(f'Test Loss: {loss:.4f}')

#Test Accuracy
print(f'Test Accuracy: {accuracy*100:.4f}')

!pip install tensorflow scikeras scikit-learn

from scikeras.wrappers import KerasClassifier
#creating a model for the keras classifier
def c_model(hidden_units=25):
  input_layer = Input(shape=(Xtrain.shape[1],))
  first_hidden = Dense(hidden_units, activation='relu')(input_layer)
  second_hidden = Dense(20, activation='relu')(first_hidden)
  third_hidden = Dense(15, activation='tanh')(second_hidden)
  output_layer = Dense(1, activation='sigmoid')(third_hidden)

  k_model = Model(inputs=input_layer, outputs=output_layer)

  k_model.compile(loss='binary_crossentropy',
              optimizer='Adam',
              metrics=['accuracy'])
  return k_model

# creating the keras Classifier object
model = KerasClassifier(build_fn=c_model,epochs=10, batch_size=32, hidden_units=32,verbose=True)

# Initialize the KFold class
cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=42)

#define hyperparameter grid
PARAMETERS = {
   'hidden_units': [32,64,128],
   'optimizer': ['adam','sgd','rmsprop'],
   'batch_size':[16,32,64],

    }

from sklearn.metrics import make_scorer

# Initialize the GridSearchCV
ker_cv = GridSearchCV(estimator= model, param_grid=PARAMETERS, cv=cv, scoring='accuracy')
gs_result=ker_cv.fit(Xtrain,Ytrain,validation_data=(x_validate, y_validate),verbose=True,callbacks=[train_hist])

# Print the best parameters and corresponding accuracy score
print("Best Parameters found are:\n ", gs_result.best_params_)

#Best model
the_best_model=gs_result.best_estimator_

#predicting the performance of the best model on the test set
y_pred = the_best_model.predict(Xtest)
pred_val=the_best_model.predict(x_validate)


#Calculation of the Accuracy Score
from sklearn.metrics import accuracy_score
test_accuracy = accuracy_score(Ytest, y_pred)
acc_validation=accuracy_score(y_validate, pred_val)
print("Test Accuracy: ", test_accuracy)
print("Validation Accuracy: ", acc_validation)

#Calculation of the AUC Score
roc_auc = roc_auc_score(Ytest, y_pred)
print("AUC on test: ", roc_auc)

"""Retrain and Retest"""

#selecting Keras best model and retraining the best model
best_params = gs_result.best_params_
final_best_model = c_model(hidden_units = best_params['hidden_units'])
final_best_model.fit(Xtrain, Ytrain, validation_data=(x_validate, y_validate),batch_size=best_params['batch_size'],verbose=True)


#Re evaluation on Test Set
y_pred_optimized = final_best_model.predict(Xtest)
y_pred_val_optimized=final_best_model.predict(x_validate)
bin_pred = (y_pred_val_optimized > 0.5).astype(int)

#Calculation of the AUC Score
roc_auc = roc_auc_score(Ytest, y_pred_optimized)
print("Retested AUC : ", roc_auc)

#Calculation of the Accuracy Score
from sklearn.metrics import accuracy_score
test_accuracy = accuracy_score(Ytest, (y_pred_optimized > 0.5).astype(int))
print("ReTest Accuracy: ", test_accuracy)
acc_revalidation=accuracy_score(y_validate, bin_pred)
print("ReValidation Accuracy: ", acc_revalidation)

#Test Loss
test_loss = final_best_model.evaluate(Xtest, Ytest)
print(f'Test Loss: {test_loss[0]:.4f}')

# check results
from sklearn.metrics import classification_report
y_pred_bin = (y_pred_optimized > 0.5).astype(int)
print(classification_report(Ytest, y_pred_bin))

"""# **Saving the Model**"""

final_best_model.save('deploy.h5')